{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e59609e6-4e46-458c-87ea-b120b7ea43f6",
   "metadata": {},
   "source": [
    "## Networks Notebook\n",
    "\n",
    "This week's notebook is very simple. We will take a set of documents (eg. posts from Instagram) and create a network based on the hashtags within it.\n",
    "\n",
    "We have some dummy data, which is some very noisy data from #foodie on Instagram, which has a lot of different languages and many messy \"communities\" within it. This is good for testing purposes, but you should use your own dataset.\n",
    "\n",
    "You should have a CSV or Excel dataset, so use the appropriate function.\n",
    "\n",
    "We have two networks to choose from, depending on what you want:\n",
    "1. Create a Hashtag-to-Hashtag network (or a co-occurrence network)\n",
    "2. Create a User-to-Hashtag network\n",
    "\n",
    "Make sure to pick the right column names from the table above and put them in the right place in the function. Also, don't forget to add the filename.\n",
    "\n",
    "Of course, I can help you create a custom network creation function.\n",
    "\n",
    "Once you have created the network, you will find a new CSV file. You can open this in Gephi using File | Import Spreadsheet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246aaa3a-cc9a-4e8a-85d6-6db57d3f4fa2",
   "metadata": {},
   "source": [
    "## Import packages and Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73ddaf64-7137-446e-bcff-365f39872de2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'caption'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/uva-drs-mental-health-assignment-BL2gK8Gv-py3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'caption'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../input_data/anxiety.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Filter out blank cells \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m df \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcaption\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misna() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/uva-drs-mental-health-assignment-BL2gK8Gv-py3.9/lib/python3.9/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/uva-drs-mental-health-assignment-BL2gK8Gv-py3.9/lib/python3.9/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'caption'"
     ]
    }
   ],
   "source": [
    "# Import network utils functions and read the dataset\n",
    "from drs_network import *\n",
    "import os\n",
    "\n",
    "df = pd.read_excel(\"../../input_data/anxiety.xlsx\")\n",
    "\n",
    "# Filter out blank cells \n",
    "df = df[df[\"caption\"].isna() == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e886c-8686-4504-9cd7-41c60dae8559",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0b84a08-12ae-4c2d-8319-394655e98ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Documents/posts column\n",
    "DOC_COL = \"body\" \n",
    "\n",
    "# User/author column\n",
    "USR_COL = \"author\"\n",
    "\n",
    "# Set topic\n",
    "TOPIC = 'network'\n",
    "\n",
    "# Set project\n",
    "PROJECT = 'anxiety'\n",
    "\n",
    "# Create a folder for the intermediate network data\n",
    "if os.path.exists(f\"../../intermediate_data/{PROJECT}_{TOPIC}\"):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(f\"../../intermediate_data/{PROJECT}_{TOPIC}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fcc98c7f-ae7d-46bd-8e6a-75e45ff02c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../intermediate_data/anxiety_network'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"../../intermediate_data/{PROJECT}_{TOPIC}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "950024a4-d784-4ec4-a604-dd3f2e0c0837",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network file saved. Open in Gephi for further processing using File | Import Spreadsheet. This is an undirected edgelist.\n"
     ]
    }
   ],
   "source": [
    "# Create a hashtag-to-hashtag network\n",
    "h2h = create_hashtag2hashtag_network(\n",
    "    list(df[DOC_COL]),\n",
    "    save_name = f\"../../intermediate_data/{PROJECT}_{TOPIC}/{PROJECT}_h2h_network.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "079cdca2-00f7-4657-82a2-88a906f6eaab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network file saved. Open in Gephi for further processing using File | Import Spreadsheet. This is a directed edgelist.\n"
     ]
    }
   ],
   "source": [
    "# Create a user-to-hashtag network\n",
    "u2h = create_user2hashtag_network(\n",
    "    df,\n",
    "    user_column = USR_COL,\n",
    "    doc_column = DOC_COL, \n",
    "    save_name = f\"../../intermediate_data/{PROJECT}_{TOPIC}/{PROJECT}_u2h_network.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebc83e2-e1e1-41ec-9e9a-8b2191f6bf77",
   "metadata": {},
   "source": [
    "## Gephi processing and file export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed0ba1-0814-43a5-baee-9d5970f7af60",
   "metadata": {},
   "source": [
    "Here I went to Gephi and ran all the statistics and then exported the Nodes table from Gephi, where I got the modularity information. \n",
    "\n",
    "You can do a lot of interesting stuff with just the network statistics in Excel, but here we will replicate Freelon's (2018) measurement of proximity.\n",
    "\n",
    "I saved my Nodes table as \"Modularity_Example.csv\". Recall that we already created a file called \"u2h_network.csv\", which we will use as our edgelist.\n",
    "\n",
    "You can also do this with the hashtag-to-hashtag network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c7655-57f3-45d3-9e08-d2b6e573ddfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute proximity matrix\n",
    "proximity_matrix = create_proximity_matrix(\n",
    "    gephi_nodes_table = \"./Modularity_Example.csv\",\n",
    "    edgelist = \"h2h_network.csv\", \n",
    "    save_name = \"h2h_proximity_matrix.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a76d09-e901-49e4-b3e5-86c7ceead299",
   "metadata": {},
   "source": [
    "There are also other ways you can slice your datat -- maybe use the partitions on users (for example) as a variable for keyness. To do that, you will have to join your data back together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
