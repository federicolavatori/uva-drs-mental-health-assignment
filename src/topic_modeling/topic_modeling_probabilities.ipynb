{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc093ab9-2ed3-4883-b21b-4baff4fb6e7f",
   "metadata": {},
   "source": [
    "# Transform data with topic probabilities\n",
    "\n",
    "Once you have selected a model that suits your data, you will then need to transform that data back on to your original data table. Then, you can pull posts by their probability in a certain topic. You will want to use Excel or Google Sheets for that, unless you learn the Pandas df.query() syntax, which is <i>relatively</i> easy. Excel filtering is a lot easier because you don't need to write any code, and it works with relatively large datasets. But working with Pandas below will certainly be much faster to get a good overview of the dataset. ChatGPT can give you a good run down on how to query a dataset with Pandas :). But make sure to indicate the DataFrame.query() method when you prompt it!\n",
    "\n",
    "A topic model doesn't simply assign a topic label to each document/post. Instead, it gives you a probability distribution of topics for that document. That means each document will have a certain probability of belonging to each topic. 0.12, or about 12%, is not very much, but usually values are lower than this. But 0.35 - 0.45 can start to be rather indicative of a topic. Usually, I start with 0.6, or about 60% likelihood of beloning to that topic to start feeling out a topic. This is just a guideline, every dataset is different and you need to experiment with these thresholds. \n",
    "\n",
    "This notebook starts by selecting a given topic model, and loading it. It only works with one k at a time! So you should first evaluate the different models before running this notebook so you know on which topic model you would like to run this computation.\n",
    "\n",
    "It is generally good practice to take note of a qualitative name for each topic, which you should create for yourself in a text or spreadsheet file. You should create a key that links the NUMBER of the topic to the NAME you created. You can use the visualisation to determine what to name each topic.\n",
    "\n",
    "So topic 4 from the visualization is actually topic 3 in the output dataset. I know this is annoying, but you need to keep it in mind when interpreting your results.\n",
    "\n",
    "First we will load the topic model and then for each of our documents, we will calculate the probability of its belonging to the k topics that the model has. \n",
    "\n",
    "Then, we will load our <b>original</b> dataset from which we created the docbins in the Corpus Linguistics notebook. Then we will run all the same filters (the Corpus Linguistics notebook dumps all blank posts because they cause an error).\n",
    "\n",
    "Then, we will join the probabilitiees we calculated in earlier to our original dataset, and export that to Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3a808-4441-4b5a-937e-dfc05723383e",
   "metadata": {},
   "source": [
    "## Step 0: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14621b15-2c20-4698-babe-bde36657d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ac1ed5-9f5c-4d1b-8685-1164a7cd072a",
   "metadata": {},
   "source": [
    "### Step 1: Load the topic model\n",
    "\n",
    "You need to enter a few variables in; this should be familiar from the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c30a548a-408b-451c-b78e-0ba457d5af87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from drs_topic_model import *\n",
    "\n",
    "# original data table path -- the one you used to make the docbins. Make sure a copy of it is in the same folder as this notebook.\n",
    "original_data_path = \"../../input_data/anxiety.xlsx\"\n",
    "\n",
    "# the column of the original data table where the text for the docbins is stored\n",
    "text_col = \"body\"\n",
    "\n",
    "# project save name -- make sure this is the same that you have been using; this is important for the naming pattern to find your topic model!\n",
    "project_save_name = \"anxiety\"\n",
    "\n",
    "#create a folder for the probabilities\n",
    "if os.path.exists(f\"../../output_data/{project_save_name}_doc_topic_probabilities\"):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(f\"../../output_data/{project_save_name}_doc_topic_probabilities\")\n",
    "\n",
    "# select the k corresponding to the topic model you feel best suits your data\n",
    "model_k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9bd45-e79b-4a81-b94d-e510305a85d8",
   "metadata": {},
   "source": [
    "### Step 2: Compute document-wise topic probability distribution\n",
    "\n",
    "Errors in the code below are likely due to issues in the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b529043-ba3a-4117-98d1-6226804f53e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlsx\n",
      "CPU times: user 37.5 s, sys: 601 ms, total: 38.1 s\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "topic_model_path = \"../../output_data/\" + project_save_name + \"_topic_models/topic_model_k_\" + str(model_k) + \"/topic_model_k_\" + str(model_k) + \".lda_mdl\"\n",
    "\n",
    "lda = gensim.models.ldamulticore.LdaMulticore\n",
    "mdl = lda.load(topic_model_path)\n",
    "\n",
    "corpus_path = f\"../../intermediate_data/{project_save_name}_topic_models_params/{project_save_name}.mm\"\n",
    "corpus = MmCorpus(corpus_path)\n",
    "\n",
    "rows = dict()\n",
    "for i, doc in enumerate(corpus):\n",
    "    row_data = {k:v for k, v in zip(range(model_k), [0 for x in range(model_k)])}\n",
    "    for k, p in mdl[doc]:\n",
    "        k_lab = \"topic_\" + str(k + 1)\n",
    "        row_data[k_lab] = p\n",
    "        rows[i] = row_data\n",
    "        \n",
    "tX = pd.DataFrame.from_dict(rows, orient=\"index\").fillna(0)\n",
    "\n",
    "df = None\n",
    "\n",
    "original_data_type = original_data_path.split(\".\")[-1]\n",
    "\n",
    "print(original_data_type)\n",
    "if original_data_type == \"xlsx\":\n",
    "    df = pd.read_excel(original_data_path)\n",
    "elif original_data_type == \"csv\":\n",
    "    df = pd.read_csv(original_data_path)\n",
    "elif original_data_type == \"pqt\":\n",
    "    df = pd.read_parquet(original_data_path)\n",
    "\n",
    "df = df[df[text_col].isna() == False]\n",
    "if len(corpus) == df.shape[0]:\n",
    "    df = df.reset_index()\n",
    "    df = df.join(tX)\n",
    "    outfile = f\"../../output_data/{project_save_name}_doc_topic_probabilities/{project_save_name}_doc_topic_probability_{model_k}.xlsx\"\n",
    "    \n",
    "    df.to_excel(outfile, index=False, engine=\"xlsxwriter\")\n",
    "else:\n",
    "    print(\"Length of corpus and length of original data table are not equal. Please consult how you created the docbins, particularly whether any filters to the dataframe were applied before your created the docbins.\")\n",
    "    print(\"Aborting computation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5bf2a-6284-4015-98c6-4630ae3f7a11",
   "metadata": {},
   "source": [
    "### Step 3: Query Data\n",
    "\n",
    "Using Pandas (our data table manipulation library), you can easily query the dataset we just created. You can use this to show the documents associated with a topic by using the correct syntax. You are using the \">\" and \"<\" operators (greater than/less then), or the \">=\" or \"<=\" operators (greater than or equal to/less than or equal to). \n",
    "\n",
    "Here is a sample query for all the rows with a topic 2 probability greater than or equal to 0.6 (60% belonging to topic 2):\n",
    "\n",
    "    df.query(\"topic_5 > 0.6\")\n",
    "    \n",
    "    # we can chain these functions following the syntax\n",
    "    df.query(\"topic_5 > 0.6\").sort_values(\"topic_5\", ascending = False) # this sorts our data largest to smallest (flip with ascending = True)\n",
    "    \n",
    "    df.query(\"topic_5 > 0.6\").sort_values(\"topic_5\", ascending = False).head(20) # give us the top 20 posts with highest probability for topic_5\n",
    "    \n",
    "    # and, if we save the query to the variable \"q\", then we can also print out the posts with the code:\n",
    "    list(q[\"caption\"]) (or whatever column you want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e19e154-f677-4146-93ae-15f71bf4cb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q = df.query(\"topic_4 > 0.6\").sort_values(\"topic_5\", ascending = False).head(5)\n",
    "list(q[\"caption\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
