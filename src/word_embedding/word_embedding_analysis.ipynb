{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j505tZTLv2xx"
   },
   "source": [
    "# Week 5 Replication of Kozlowski et al. \n",
    "\n",
    "This is a tutorial to teach you about how word embeddings work and how they can be used to explore the documents we've created. On the whole, you need a large amount of data to train reliable word embeddings. This notebook is more about showing you how useful they can be to understand bias, and make sense of Kozlowski et al. I hoped that larger scale data collection would have been possible, but most of you are working with smaller datasets. Consequently this technique might not be useful now, but it may come in handy during the data sprint.\n",
    "\n",
    "It is a very useful technique to understand bias in text, which is the theme that this notebook is organized around.\n",
    "\n",
    "This notebook is a bit advanced, but it will help you make sense of what Kozlowski et al have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8595,
     "status": "ok",
     "timestamp": 1678183294999,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "D5laer-dPUFX",
    "outputId": "e71dd544-42c2-4ae0-fa59-2ea21637304c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from drs_word_embedding import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqpN-M4kXGHu"
   },
   "source": [
    "# **Let's play around with word embeddings**\n",
    "\n",
    "The reading discussed what word embeddings are, and you may have checked out a few Github pages that have some nice explainers on what word embeddings are.\n",
    "\n",
    "For our class, we want to look at how word embeddings might tell us something about bias, inequality, and related issues such as injustice, sexism, and racism.\n",
    "\n",
    "There are two way you can work with word embeddings. You can explore existing models that have been trained on certain kinds of corpora. For example, immediately below we use a Glove model trained on Wikipedia to explore how to project embeddings onto a 2-D plane for visualisation. These are **pretrained** models. You can, for example, compare embeddings from a Glove model trained on Wikipedia with one trained on Google News (though this might not be that interesting!).\n",
    "\n",
    "The second, more interesting, way to work with word embeddings is to create your own and compare it to others. You can use another notebook to do that.\n",
    "\n",
    "In this notebook, I provide you with the code to\n",
    "\n",
    "1. Load a pretrained word embedding model and visualise its outputs\n",
    "2. Replicate (roughly) Kozlowski et al. (2019)'s projection methods. Projection here refers to **plotting selected embeddings on a 2-D space.**\n",
    "3. Make a basic plot of projected word embeddings. We use matplotlib here to do the plotting.\n",
    "4. Load your own corpus and make a word embedding model using the word2vec algorithm\n",
    "5. Use Kozlowski et al.'s projection technique to compare word embeddings between a test model (in my example, subreddits) and a reference model (in my example, the Glove Wikipedia model).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9D9Vfhm3dGnm"
   },
   "source": [
    "## **Part 1: Load a pretrained model & Project Embeddings**\n",
    "\n",
    "Gensim, the package we use for the word2vec algorithm, also has a lot of pretrained models for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 758,
     "status": "ok",
     "timestamp": 1678183295754,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "dM0zVsQVV4nl",
    "outputId": "7d5eacf7-5b90-465b-d680-85ece0bb9295",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Let's look at all the available Gensim packages\n",
    "\n",
    "info = api.info()\n",
    "for model_name, model_data in sorted(info['models'].items()):\n",
    "    print(\n",
    "        '%s (%d records): %s' % (\n",
    "            model_name,\n",
    "            model_data.get('num_records', -1),\n",
    "            model_data['description'][:40] + '...',\n",
    "        )\n",
    "    )\n",
    "\n",
    "## its not super pretty but you can check out the models below\n",
    "## we are going to use glove-wiki-gigaword-50\n",
    "## check out Gensim's documentation for more information on these packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2u4KcfZdwQz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Let's get the glove-wiki-gigaword-50 model\n",
    "\n",
    "model = api.load(\"glove-wiki-gigaword-50\") # you can use any of the models you like; just make sure you copy/paste the string correctly\n",
    "\n",
    "## it might need to download, so grab a coffee...\n",
    "## once it is done, you've loaded a word embedding model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKD5fwf1oC1b"
   },
   "source": [
    "## Exploring a Word Embedding Model\n",
    "\n",
    "As Kozlowski et al (2019) discuss, a word embedding model is basically a representation of words and their relationships to one another in a high-dimensional space. This is done by using vectors, in this case a mathematical representation of a word. Each word in our model has a specific vector. These words can then by analysed by using a metric that calculates the distance between two vectors. In embedding models, the *cosine distance* between vector A and vector B is the metric typically used.\n",
    "\n",
    "Let's do some basic operations with the model to get a sense of how it works. Check out the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1678183333072,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "LwTFxG8cpjbL",
    "outputId": "650d14a8-7884-4d50-bf0d-d08406a97467",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## we loaded the model above with api.load()\n",
    "## so now we have working Word2Vec model\n",
    "## we are using the Word2Vec class from Gensim, there's plenty of documentation\n",
    "\n",
    "## Lets see what a word vector looks like.\n",
    "\n",
    "model[\"king\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1678183333072,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "Zf5hY8kbpjYv",
    "outputId": "31a7f42b-cf44-4689-8874-a0aecf4af32c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Lets see another word vector\n",
    "\n",
    "model[\"man\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1678183333073,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "fvfon9x_pjV-",
    "outputId": "b07c87a7-4e4b-462b-c137-cc1591d106c2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## clearly, these are different words with different vectors\n",
    "## obviously this makes little sense to a human eye. \n",
    "## we probably want to know how similar these terms are\n",
    "\n",
    "## let's calculate the cosine distance between these two terms\n",
    "\n",
    "model.distance(\"king\",\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1678183333074,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "mAZJBZb-pjT1",
    "outputId": "9c5ca3c7-0334-49cf-e762-c10f00ebd330",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## OK -- these words have a distance of 0.47\n",
    "## What about the distance between \"king\" and \"woman\"?\n",
    "\n",
    "model.distance(\"king\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1678183333074,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "bd-qh-ArpjQx",
    "outputId": "0a26f213-a4b4-4148-da41-c844ab95e605",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Hey! They have a higher difference. That means that MAN is closer to KING than WOMAN is to KING. Makes sense because the noun KING in English is not gender neutral\n",
    "## We might also want to find all of the words that are similar to KING\n",
    "\n",
    "model.most_similar(\"king\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1678183333075,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "2Il_x0fdpjOM",
    "outputId": "08ed04f0-1054-46b6-ea82-f2facb2900d7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## These are words obviously relating to rulers\n",
    "## But we can also add and subtract vectors\n",
    "## In theory, if we subtract the vector for MAN from KING, and add the vector for WOMAN, we should end up with the word QUEEN and related terms\n",
    "\n",
    "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=50)\n",
    "\n",
    "## Some notes about syntax with this command\n",
    "## .most_similar() allows you to simply compare two words\n",
    "## you can use postive for the vectors you want to add together and negative for the vectors you want to subtract\n",
    "## you can also add the argument topn=N to get more results. Here we ask for the top 50 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1678183333384,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "QXM1wn7DpjKT",
    "outputId": "7b2d044b-c393-4041-fc77-10ced8fd2517",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## let's try something else\n",
    "\n",
    "mdist = model.distance(\"president\",\"man\")\n",
    "wdist = model.distance(\"president\",\"woman\")\n",
    "\n",
    "print(\"Distance, president --> man: \", mdist)\n",
    "print(\"Distance, president --> woman: \", wdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1678183333384,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "vSSUjFAQpjHk",
    "outputId": "dbb350bc-1daf-4754-a169-af70a1ba3e72",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## So English tends to associate men with the term president MORE than women. Big surprise.\n",
    "## add some code cells below and try out some distances between words\n",
    "\n",
    "model.distance(\"refugee\",\"european\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1678183333385,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "5dCBjyVzpjFh",
    "outputId": "1465621c-a091-4676-9bcb-233b8e592e65",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.distance(\"refugee\",\"african\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.distance(\"refugee\",\"jewish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.distance(\"refugee\",\"asian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.distance(\"refugee\",\"american\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEEQItyupi43",
    "tags": []
   },
   "source": [
    "So English wikipedia seems to think Europeans are less associated with being refugees with other groups except Asians and Americans\n",
    "\n",
    "This reflects a Eurocentric geopolitical perspective that the model recognizes in English Wikipedia texts. But to be certain of anything, we would need to run many more tests, and visualise these biases somehow.\n",
    "\n",
    "We can do that by replicating Kozlowski et al's approach. In this manner, we can explore biases in a textual dataset. \n",
    "\n",
    "Add some more queries in code cells below, being careful to follow the syntax, and lowercasing your words (for this model).\n",
    "\n",
    "From the perspective the data used to train the model, we might be able to observe the symbolic spaces at particular boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.distance(\"government\",\"evil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.distance(\"govenment\",\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Interesting, but we can only glean so much from one query! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtNT17mfuMYN"
   },
   "source": [
    "**So what have we learned so far?**\n",
    "\n",
    "1. Once a Word2Vec model is trained, every word in a corpus has vector which can be compared to another.\n",
    "2. calling .distance(\"word1\",\"word2\") will calculate the cosine distance between each word's vectors. A higher value means they are more distant, while a lower value means they are more similar.\n",
    "3. calling .most_similar() will return N words most similar to your query. This is also based on cosine distance, but the value returned is different. In this case, higher values are MORE SIMILAR and lower values are LESS SIMILAR.\n",
    "\n",
    "Note: calling .most_similar() returns a list of python tuples, which is easy to convert to a data table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSrxVbHgvy4l"
   },
   "source": [
    "## **Part 2: Reverse Engineering Projection**\n",
    "\n",
    "It's not much fun when you just have numbers to work with. So how do we visualise these embeddings in a meaningful way?\n",
    "\n",
    "Fortunately, Kozlowski et al (2019) have an approach that we can **roughly** replicate. What they do is they select anotnym pairs, for example, \"rich\"/\"poor\" and \"man\"/\"woman\" as axes. Then they plot a set of neutral words on a 2-D plane formed by these axes. \n",
    "\n",
    "\n",
    "**The naive version**\n",
    "\n",
    "So, we can replicate a **naive** version of their projection approach.\n",
    "\n",
    "Let's assume we wish to plot two neutral words, \"engineer\" and \"nurse\" on an axis \"man\"/\"woman\".\n",
    "\n",
    "That means we need to calculate the distance between \"man\" and \"engineer\" and \"woman\" and \"engineer\". But we need to project that onto the axis. So what we can do is simply calculate the difference between the cosine distance of (man, engineer) and cosine distance of (woman, engineer). So basically, we use this formula:\n",
    "\n",
    "     x = model.distance(\"man\",\"engineer\") - model.distance(\"woman\", \"engineer\")\n",
    "\n",
    "Where x is simply the x-value for the word engineer if the x-axis goes from man to woman.\n",
    "\n",
    "Then we just do the same with \"nurse\". We will have two different x-values for each word. We would expect that the x-value for engineer is lower than nurse, meaning that engineer is more closely associated with men in our model.\n",
    "\n",
    "We *could* plot this, but we need another axis for a 2-D plane. Let's use rich/poor. We do the same thing, except for the y-axis\n",
    "\n",
    "     y = model.distance(\"rich\",\"engineer\") - model.distance(\"poor\",\"engineer\")\n",
    "\n",
    "Where y is simply the y-value for the word engineer on the rich/poor (y) axis.\n",
    "\n",
    "So now we have actual coordinates that we can plot! Engineer, x, y vs Nurse, x, y. \n",
    "\n",
    "The code cells below basically do this for you, so you don't need to create your own function. But read through to get a sense of how it works.\n",
    "\n",
    "You need to have the following to project words onto these axes:\n",
    "1. A word2vec model\n",
    "2. Two opposing words for your x axis (eg. man/woman)\n",
    "3. Two opposing words for your y axis (eg. rich/poor)\n",
    "4. A set of test words, eg names of jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "executionInfo": {
     "elapsed": 626,
     "status": "ok",
     "timestamp": 1678183334004,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "RT7swg3GVVQh",
    "outputId": "5a9cf752-994b-4b51-f763-65a17e07fa6f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## OK, now lets use the naive projection \n",
    "## first set up your x and y axes\n",
    "\n",
    "x_axis = [\"man\",\"woman\"]\n",
    "y_axis = [\"rich\",\"poor\"]\n",
    "test_words = [\"custodian\",\"player\",\"cleaner\",\"dentist\",\"secretary\",\"dancer\",\"professor\",\"engineer\",\"driver\",\"teacher\",\"nurse\",\"doctor\",\"lecturer\",\"lawyer\",\"paralegal\",\"athlete\",\"criminal\",\"unemployed\"]\n",
    "\n",
    "naive_projection(x_axis, y_axis, test_words, model,  plot_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_HppR2uWhyV"
   },
   "source": [
    "# Improving the projection\n",
    "\n",
    "Recall that we referred to this projection method as naive. It's not bad, but it could be better. That's because its not *really* what Kozlowski et al. do.\n",
    "\n",
    "Instead, they take a large set of word pairs for each axis--for example, not only man/woman, but also he/she and male/female. All of these are related to the concept man/woman, but are different forms that mean something similar. Let's call an axis where we merge multiple, related antonymic pairs a **composite axis**.\n",
    "\n",
    " Then, they calculate the distances like we did using the formulas above, but take the average across antonym pairs. \n",
    "\n",
    "So what they do is similar to the following, for the test word \"engineer\" across multiple axes (man/woman, male/female, he/she):\n",
    "\n",
    "    x1 = model.distance(\"man\",\"engineer\") - model.distance(\"woman\",\"engineer\")\n",
    "    x2 = model.distance(\"he\",\"engineer\") - model.distance(\"she\",\"engineer\")\n",
    "    x3 = model.distance(\"male\",\"engineer\") - model.distance(\"female\",\"engineer\")\n",
    "\n",
    "But now we have three x values for engineer! We can't plot that. So both research teams simply took the average of these three x values to get one combined x value.\n",
    "\n",
    "    x_val = average(x1, x2, x3) # this is psuedocode, won't work immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1678183334005,
     "user": {
      "displayName": "B. Ganesh",
      "userId": "08473345295257536056"
     },
     "user_tz": -60
    },
    "id": "oq-YF7N-YR8M",
    "outputId": "899ae885-9c32-494e-e28b-5800287ff902",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## calculate the x values for our 'composite' axes for the word engineer\n",
    "x1 = model.distance(\"man\",\"engineer\") - model.distance(\"woman\",\"engineer\")\n",
    "x2 = model.distance(\"he\",\"engineer\") - model.distance(\"she\",\"engineer\")\n",
    "x3 = model.distance(\"male\",\"engineer\") - model.distance(\"female\",\"engineer\")\n",
    "\n",
    "## note, statistics.mean() only takes a python list as an input, so you have to pass the values in as a list, with square brackets\n",
    "x_eng = statistics.mean([x1, x2, x3])\n",
    "\n",
    "## calculate the x values for our 'composite' axes for the word dancer\n",
    "x1 = model.distance(\"man\",\"dancer\") - model.distance(\"woman\",\"dancer\")\n",
    "x2 = model.distance(\"he\",\"dancer\") - model.distance(\"she\",\"dancer\")\n",
    "x3 = model.distance(\"male\",\"dancer\") - model.distance(\"female\",\"dancer\")\n",
    "\n",
    "## get the average, our x value for dancer on our composite axis\n",
    "x_dnc = statistics.mean([x1, x2, x3])\n",
    "\n",
    "print(\"x-value for engineer: \", x_eng)\n",
    "print(\"x-value for dancer: \", x_dnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## now, lets set up our composite axes\n",
    "## you can put in any words you want, but note the format and the location of square brackets\n",
    "## here we are hard-coding the dimensions. We could theoretically import them from an external dataframe...\n",
    "\n",
    "x_dimensions = [\n",
    "                [\"man\",\"woman\"],\n",
    "                [\"male\",\"female\"],\n",
    "                [\"he\",\"she\"],\n",
    "                [\"father\",\"mother\"],\n",
    "                [\"him\",\"her\"]\n",
    "] # NOTE: this needs to be a list of lists, and your nested lists should only have ONE pair. same with y.\n",
    "\n",
    "y_dimensions = [\n",
    "                [\"rich\",\"poor\"],\n",
    "                [\"wealthy\",\"impoverished\"],\n",
    "                [\"affluent\",\"destitute\"],\n",
    "] # check out the appendices of Kozlowski et al and you will that they used loads of words here\n",
    "# you can do as many dimensions as you want in your composite axis. check out a thesaurus!\n",
    "\n",
    "## these are the same as before\n",
    "## change to whatever test words you want! just make sure to follow the formatting\n",
    "test_words = [\n",
    "    \"custodian\",\n",
    "    \"player\",\n",
    "    \"cleaner\",\n",
    "    \"dentist\",\n",
    "    \"secretary\",\n",
    "    \"dancer\",\n",
    "    \"professor\",\n",
    "    \"engineer\",\n",
    "    \"driver\",\n",
    "    \"teacher\",\n",
    "    \"nurse\",\n",
    "    \"doctor\",\n",
    "    \"lecturer\",\n",
    "    \"lawyer\",\n",
    "    \"paralegal\",\n",
    "    \"athlete\",\n",
    "    \"criminal\",\n",
    "    \"unemployed\"\n",
    "]\n",
    "\n",
    "\n",
    "## These are your axis labels, make them whatever you like\n",
    "\n",
    "xlab = \"man < 0 < woman\"\n",
    "ylab = \"rich < 0 < poor\"\n",
    "\n",
    "## call the function, make your plot\n",
    "\n",
    "advanced_projection(\n",
    "    x_dimensions,\n",
    "    y_dimensions,\n",
    "    test_words,\n",
    "    model,\n",
    "    plot_size=8,\n",
    "    xlab = xlab,\n",
    "    ylab = ylab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJCYSrLSf_gC"
   },
   "source": [
    "### **Now this looks better! (from a projection perspective, the biases are still evident of course!)**\n",
    "\n",
    "You can play with some composite axes, words, etc. on the Glove wikipedia vectors to look at whether there is some bias. Note that very small changes in the dimensions on the axes (adding or subtracting a pair) have a significant impact on the visualization. \n",
    "\n",
    "Try to come up with a different experiment about bias in English wikipedia, for example, looking at different countries and words about geopolitics, or music genres and class categories like Kozlowski et al do.\n",
    "\n",
    "However, you could go further. For example, you could download and load a different model, and run all of these code cells again, and compare the results. Maybe Glove Wikipedia has less bias than Glove Twitter. That's interesting in itself.\n",
    "\n",
    "This is the end of the tutorial, and you can move on to the next notebook to train your own w2v model, but remember they are not so useful for small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lSrxVbHgvy4l",
    "kJCYSrLSf_gC"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
