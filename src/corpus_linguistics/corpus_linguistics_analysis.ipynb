{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017cb975-9631-4546-8739-908ebe32365f",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05a2eacb-8e50-4455-b8cc-9577019c7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from drs_corpora import * #import helper functions\n",
    "nlp = spacy.load(\"en_core_web_sm\") # load the spaCy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb55ef-26ff-40cd-9a25-8b557f9984cf",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69784020-fd2b-47f4-a63e-bc4d800b6d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to docbins folder: ../../intermediate_data/anxiety_docbins\n"
     ]
    }
   ],
   "source": [
    "# Set some parameters\n",
    "PROJECT = 'anxiety'\n",
    "DOCBIN_SIZE = None\n",
    "SAVEDIR = f\"../../intermediate_data/{PROJECT}_docbins\"\n",
    "DOCBIN_FILENAME_PATTERN = SAVEDIR + \"/\" + PROJECT\n",
    "\n",
    "# Create a folder for the docbins of your project within intermediate_data if not exists\n",
    "try:\n",
    "    os.mkdir(SAVEDIR)\n",
    "    print(\"Directory created. Path to docbins folder:\", SAVEDIR)\n",
    "except FileExistsError:\n",
    "    print(\"Path to docbins folder:\", SAVEDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c74e19-22dc-4269-bd4e-44e02f16c5fa",
   "metadata": {},
   "source": [
    "## Read Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a13e986e-73aa-48ea-a3b2-d87fda4485d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the folder containing CSV files\n",
    "input_path = '../../input_data/'\n",
    "\n",
    "# Create a DataFrame with the accounts and the groups they belong to (professional, experience)\n",
    "account_groups_df = pd.read_json(f\"{input_path}account_groups.json\", typ='series').reset_index().rename(columns={\"index\": \"account\", 0: \"category\"})\n",
    "\n",
    "# Initialize an empty list to store the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(input_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Construct the full path to the CSV file\n",
    "        file_path = os.path.join(input_path, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Filter non-null posts\n",
    "df = df[df[\"body\"].isna() == False]\n",
    "documents = list(df[\"body\"])\n",
    "\n",
    "# Save to Excel\n",
    "#df.to_excel(f\"{input_path}anxiety_test.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8efb92-3398-46cc-bf0a-0660cb70eaa3",
   "metadata": {},
   "source": [
    "## Tokenization, Lemmatization, Named Entity Recognition, and POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a55cacb-93a5-4491-a177-0740f6315b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docbin size for this project:  1000 \n",
      "Please take note of the docbin size if you want to come back to your project and not process the data again.\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_0.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_1.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_2.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_3.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_4.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_5.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_6.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_7.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_8.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_9.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_10.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_11.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_12.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_13.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_14.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_15.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_16.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_17.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_18.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_19.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_20.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_21.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_22.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_23.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_24.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_25.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_26.db\n",
      "CPU times: user 1min 56s, sys: 18.1 s, total: 2min 14s\n",
      "Wall time: 8min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp.add_pipe(\"merge_entities\") # this makes the tokens cleaner later\n",
    "\n",
    "#we split up all the documents into a bunch of \".db\" files, or \"docbin\" files that hold our text data.\n",
    "\n",
    "N_PROCESS = 10 # you can change this depending on how many cores your CPU has (eg. a Mac M1 has 8 cores, so you can use up to 7 here)\n",
    "BATCH_SIZE = 100 # this depends on how much RAM you have. If this process hangs or crashes, you may want to reduce batch size (how many docs each core will process in one chunk)\n",
    "DOCBIN_SIZE = N_PROCESS * BATCH_SIZE\n",
    "\n",
    "print(\"docbin size for this project: \", str(DOCBIN_SIZE), \"\\nPlease take note of the docbin size if you want to come back to your project and not process the data again.\")\n",
    "\n",
    "for i, chunk in enumerate(chunker(documents, DOCBIN_SIZE)): # chopping our dataset into chunks. We don't need this in our toy example, but we do for large datsets; change from 100 if you want\n",
    "    doc_bin = DocBin(store_user_data = True) # create a docbin for our chunk\n",
    "    for doc in nlp.pipe(chunk, n_process = N_PROCESS, batch_size = BATCH_SIZE): # process our documents, you can play with n_process and batch_size depending on your CPU and RAM\n",
    "        doc_bin.add(doc) # save the document to our docbin\n",
    "    chunk_name = DOCBIN_FILENAME_PATTERN + \"_docbin_\" + str(i) + \".db\" # make a nice filename for each chunk\n",
    "    print(\"Saving chunk as: \", chunk_name) # display progress\n",
    "    doc_bin.to_disk(chunk_name) # save docbin for chunk to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9ba50-2e29-4892-88db-4d466227a971",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4b505c0-dd40-440a-ba2c-4e231d89fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we can do anything, we need to get a list of all the names of the docbins, in order, thats what happens here\n",
    "\n",
    "docbin_folder = SAVEDIR + \"/*.db\" # you might have to change this if you use this notebook for a different folder\n",
    "docbins = [docbin for docbin in glob.iglob(docbin_folder)]\n",
    "DOCBINS = list()\n",
    "for i in range(0, len(docbins), 1):\n",
    "    db_path = DOCBIN_FILENAME_PATTERN + \"_docbin_\" + str(i) + \".db\" # here the naming pattern of the docbins is hard coded, so you may have to change this if you apply it to another project\n",
    "    DOCBINS.append(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41bef69a-a218-44cf-9633-64b37658c582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency distribution saved!\n"
     ]
    }
   ],
   "source": [
    "# Let's do the counting!\n",
    "\n",
    "subtotals = [docbin_counter(docbin, nlp) for docbin in DOCBINS] # we will apply a counting function to each docbin here\n",
    "total = Counter() # we set up a blank counter which will consolidate all the docbin-level totals\n",
    "for subtotal in subtotals: # this loop does the counting\n",
    "    total.update(subtotal)\n",
    "fdist = fdist2table(total, savename = \"../../output_data/words_frequency.xlsx\") # save it to excel, you can make the filename whatever you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "806b2014-27f5-49aa-89ec-51ebb1f68615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>31711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>life</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>23492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11786</th>\n",
       "      <td>judgement</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>16500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>growth</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>16056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14240</th>\n",
       "      <td>⁣</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>13235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>health</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>12942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40507</th>\n",
       "      <td>keepgoing</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>11238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13480</th>\n",
       "      <td>comprehension</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>10967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>thing</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>8323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>#mentalhealth #</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>8214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1525</th>\n",
       "      <td>mentalhealthawarenes</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>7849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3347</th>\n",
       "      <td>depression</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>7750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>time</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>7609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4102</th>\n",
       "      <td>happiness</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>7053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>thought</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>6921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3962</th>\n",
       "      <td>gratitude</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>6606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>inspiration</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>6540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14049</th>\n",
       "      <td>loveyourself</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>6530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32822</th>\n",
       "      <td>believeinyourself</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>6395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25563</th>\n",
       "      <td>mentalhealthblogger</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>6293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       word label  count\n",
       "48                  anxiety  NOUN  31711\n",
       "575                    life  NOUN  23492\n",
       "11786             judgement  NOUN  16500\n",
       "3464                 growth  NOUN  16056\n",
       "14240                     ⁣  NOUN  13235\n",
       "9                    health  NOUN  12942\n",
       "40507             keepgoing  NOUN  11238\n",
       "13480         comprehension  NOUN  10967\n",
       "57                    thing  NOUN   8323\n",
       "1782        #mentalhealth #  NOUN   8214\n",
       "1525   mentalhealthawarenes  NOUN   7849\n",
       "3347             depression  NOUN   7750\n",
       "47                     time  NOUN   7609\n",
       "4102              happiness  NOUN   7053\n",
       "207                 thought  NOUN   6921\n",
       "3962              gratitude  NOUN   6606\n",
       "316             inspiration  NOUN   6540\n",
       "14049          loveyourself  NOUN   6530\n",
       "32822     believeinyourself  NOUN   6395\n",
       "25563   mentalhealthblogger  NOUN   6293"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show top 20 NOUN, VERB, ADJ in a table\n",
    "fdist.query(\"label == 'NOUN'\").sort_values(\"count\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d467c49-2246-43f9-8e70-62c000b69b88",
   "metadata": {},
   "source": [
    "## Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24e2a207-ad7a-41b0-beb4-a3926bb737f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered, printing contingency table values...\n",
      " A: 3  B: 23489  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 4  B: 23488  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 3  B: 23489  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 3  B: 23489  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 3  B: 0  C: 23489  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 3  B: 0  C: 23489  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 4  B: 23488  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 25  B: 23467  C: -7  D: 1582927\n",
      "Error encountered, printing contingency table values...\n",
      " A: 5  B: 0  C: 23487  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 6  B: 0  C: 23486  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 6  B: 23486  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 20  B: 23472  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 5  B: 0  C: 23487  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 4  B: 23488  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 11  B: 23481  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 3  B: 23489  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 3  B: 23489  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 4  B: 23488  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 5  B: 23487  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 3  B: 23489  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 4  B: 23488  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 3  B: 0  C: 23489  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 4  B: 23488  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 8  B: 0  C: 23484  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 8  B: 23484  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 5128  B: 18364  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 2274  B: 21218  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 13  B: 0  C: 23479  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 15  B: 0  C: 23477  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 13  B: 0  C: 23479  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 33  B: 23459  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 8  B: 0  C: 23484  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 6  B: 23486  C: 0  D: 1582920\n",
      "Error encountered, printing contingency table values...\n",
      " A: 12  B: 23480  C: 0  D: 1582920\n"
     ]
    }
   ],
   "source": [
    "cl_df = collocator_main(\n",
    "    \n",
    "    (\"life\", \"NOUN\"),   ## you can change this, but pay attention to the format!\n",
    "    DOCBINS, \n",
    "    nlp, \n",
    "    total, \n",
    "    window_size = 3,   ## you can change this, we will discuss this in class (window size = 2 -> just next to the one we have)\n",
    "    remove_stopwords = True ## option for advanced use\n",
    "    \n",
    ")\n",
    "\n",
    "# this will look a little clunky, and as you can see, some minor errors need fixing; this is the most complicated computation.\n",
    "\n",
    "cl_df.to_excel(\"../../output_data/words_collocation.xlsx\", index=True, engine = \"xlsxwriter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a88ba9-b8ec-424c-8310-0d4348444b12",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc0065e5-f2a5-48f5-97ac-0ea60fcac5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Concordance generated for comprehension|NOUN. Click <a href = '../../output_data/comprehension|NOUN_concordance.html' target = '_blank'>here</a> to view.</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_hits = concordancer(\n",
    "    DOCBINS,\n",
    "    (\"comprehension\",\"NOUN\"),\n",
    "    5, # this is how on either side to look for (window size)\n",
    "    nlp,\n",
    "    sample_size = 20,\n",
    "    label = \"comprehension|NOUN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc94f75-8977-46eb-88be-f461343918ab",
   "metadata": {},
   "source": [
    "#### Observe: apparently most keywords are only used as hashtgas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24ded7-edc0-4648-8872-f937d21777c8",
   "metadata": {},
   "source": [
    "## Keyness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31a2c9a5-d828-4fc8-b711-291a9eaf0f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting words in docbins...\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_0.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_1.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_2.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_3.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_4.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_5.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_6.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_7.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_8.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_9.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_10.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_11.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_12.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_13.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_14.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_15.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_16.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_17.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_18.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_19.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_20.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_21.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_22.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_23.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_24.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_25.db\n",
      "../../intermediate_data/anxiety_docbins/anxiety_docbin_26.db\n"
     ]
    }
   ],
   "source": [
    "# To compute keyness, first we need a frequency distribution of a subset of our posts to compare with the total.\n",
    "\n",
    "keyness_fdist = sliced_docbin_word_counter(\n",
    "    \n",
    "    DOCBINS,\n",
    "    df,\n",
    "    nlp,\n",
    "    slice_value = \"healthanxietycoach\", # we are slicing on a value, so here we can put in the username we want\n",
    "    slice_variable = \"author\", # this tells us which column of our data table to find the value above\n",
    "    remove_stopwords = True,\n",
    "    docbin_size = DOCBIN_SIZE\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc3b8312-2a26-416e-8aac-54141f4ddbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then it is just a matter of statistics! We will use a Chi-Squared statistic and the PDIFF statistic proposed by Gabrielatos (2018)\n",
    "\n",
    "kn_df = keyness_chi_sq(keyness_fdist, total, savename = \"../../output_data/keywords_chisq.xlsx\")\n",
    "kn_df = keyness_pdiff(keyness_fdist, total, savename = \"../../output_data/keywords_pdiff.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c871b-f453-4e1b-b4b3-995d96364006",
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_df.sort_values(\"pdiff\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596c28e-ac19-4a3b-b8ee-cd168066a93c",
   "metadata": {},
   "source": [
    "## Hashtag and Social Media Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4188ebe4-4a75-4dc7-8b0b-4c445cd4a1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashtag frequency distribution saved!\n"
     ]
    }
   ],
   "source": [
    "ht_fdist = hashtag_counter(documents, savename = \"../../output_data/hashtags.xlsx\") # this takes care of everything and makes an excel spreadsheet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
