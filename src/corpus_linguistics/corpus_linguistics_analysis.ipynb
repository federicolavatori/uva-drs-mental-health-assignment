{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017cb975-9631-4546-8739-908ebe32365f",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a2eacb-8e50-4455-b8cc-9577019c7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from drs_corpora import * #import helper functions\n",
    "nlp = spacy.load(\"en_core_web_sm\") # load the spaCy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb55ef-26ff-40cd-9a25-8b557f9984cf",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69784020-fd2b-47f4-a63e-bc4d800b6d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to docbins folder: ../../intermediate_data/anxiety_docbins\n"
     ]
    }
   ],
   "source": [
    "# Set some parameters\n",
    "PROJECT = 'anxiety'\n",
    "DOCBIN_SIZE = None\n",
    "SAVEDIR = f\"../../intermediate_data/{PROJECT}_docbins\"\n",
    "DOCBIN_FILENAME_PATTERN = SAVEDIR + \"/\" + PROJECT\n",
    "\n",
    "# Create a folder for the docbins of your project within intermediate_data if not exists\n",
    "try:\n",
    "    os.mkdir(SAVEDIR)\n",
    "    print(\"Directory created. Path to docbins folder:\", SAVEDIR)\n",
    "except FileExistsError:\n",
    "    print(\"Path to docbins folder:\", SAVEDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c74e19-22dc-4269-bd4e-44e02f16c5fa",
   "metadata": {},
   "source": [
    "## Read Raw Data and create professional vs non-professional dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abe7be66-417a-4710-a77e-26144b890939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the folder containing CSV files\n",
    "input_path = '../../input_data/'\n",
    "\n",
    "# Create a DataFrame with the accounts and the groups they belong to (professional, experience)\n",
    "account_groups_df = pd.read_json(f\"{input_path}account_groups.json\", typ='series').reset_index().rename(columns={\"index\": \"account\", 0: \"category\"})\n",
    "\n",
    "# Initialize an empty list to store the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(input_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Construct the full path to the CSV file\n",
    "        file_path = os.path.join(input_path, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Filter non-null posts\n",
    "df = df[df[\"body\"].isna() == False]\n",
    "\n",
    "# Save to Excel\n",
    "#df.to_excel(f\"{input_path}anxiety.xlsx\")\n",
    "\n",
    "# Merge to accounts info\n",
    "df = df.merge(account_groups_df, left_on = 'author', right_on = 'account')\n",
    "\n",
    "# Create and save 2 dfs\n",
    "professional_df = df[df['category'] == 'professional']\n",
    "#professional_df.to_excel(f\"{input_path}anxiety_professional.xlsx\")\n",
    "\n",
    "experience_df = df[df['category'] == 'experience']\n",
    "#experience_df.to_excel(f\"{input_path}anxiety_experience.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "556d8b86-5648-4d3c-a4cd-9c2b43560f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13588"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experience_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6124c2db-71ca-49b9-b36e-876c43e82ea7",
   "metadata": {},
   "source": [
    "## Pick a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "beaa756b-31ff-4249-bb95-f774c88f590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = experience_df\n",
    "#df = professional_df\n",
    "documents = list(df[\"body\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8efb92-3398-46cc-bf0a-0660cb70eaa3",
   "metadata": {},
   "source": [
    "## Tokenization, Lemmatization, Named Entity Recognition, and POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6a55cacb-93a5-4491-a177-0740f6315b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docbin size for this project:  1000 \n",
      "Please take note of the docbin size if you want to come back to your project and not process the data again.\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_0.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_1.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_2.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_3.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_4.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_5.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_6.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_7.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_8.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_9.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_10.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_11.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_12.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_13.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_14.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_15.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_16.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_17.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_18.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_19.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_20.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_21.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_22.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_23.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_24.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_25.db\n",
      "Saving chunk as:  ../../intermediate_data/anxiety_docbins/anxiety_docbin_26.db\n",
      "CPU times: user 1min 46s, sys: 20.1 s, total: 2min 6s\n",
      "Wall time: 7min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp.add_pipe(\"merge_entities\") # this makes the tokens cleaner later\n",
    "\n",
    "#we split up all the documents into a bunch of \".db\" files, or \"docbin\" files that hold our text data.\n",
    "\n",
    "N_PROCESS = 10 # you can change this depending on how many cores your CPU has (eg. a Mac M1 has 8 cores, so you can use up to 7 here)\n",
    "BATCH_SIZE = 100 # this depends on how much RAM you have. If this process hangs or crashes, you may want to reduce batch size (how many docs each core will process in one chunk)\n",
    "DOCBIN_SIZE = N_PROCESS * BATCH_SIZE\n",
    "\n",
    "print(\"docbin size for this project: \", str(DOCBIN_SIZE), \"\\nPlease take note of the docbin size if you want to come back to your project and not process the data again.\")\n",
    "\n",
    "for i, chunk in enumerate(chunker(documents, DOCBIN_SIZE)): # chopping our dataset into chunks. We don't need this in our toy example, but we do for large datsets; change from 100 if you want\n",
    "    doc_bin = DocBin(store_user_data = True) # create a docbin for our chunk\n",
    "    for doc in nlp.pipe(chunk, n_process = N_PROCESS, batch_size = BATCH_SIZE): # process our documents, you can play with n_process and batch_size depending on your CPU and RAM\n",
    "        doc_bin.add(doc) # save the document to our docbin\n",
    "    chunk_name = DOCBIN_FILENAME_PATTERN + \"_docbin_\" + str(i) + \".db\" # make a nice filename for each chunk\n",
    "    print(\"Saving chunk as: \", chunk_name) # display progress\n",
    "    doc_bin.to_disk(chunk_name) # save docbin for chunk to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9ba50-2e29-4892-88db-4d466227a971",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4b505c0-dd40-440a-ba2c-4e231d89fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we can do anything, we need to get a list of all the names of the docbins, in order, thats what happens here\n",
    "\n",
    "docbin_folder = SAVEDIR + \"/*.db\" # you might have to change this if you use this notebook for a different folder\n",
    "docbins = [docbin for docbin in glob.iglob(docbin_folder)]\n",
    "DOCBINS = list()\n",
    "for i in range(0, len(docbins), 1):\n",
    "    db_path = DOCBIN_FILENAME_PATTERN + \"_docbin_\" + str(i) + \".db\" # here the naming pattern of the docbins is hard coded, so you may have to change this if you apply it to another project\n",
    "    DOCBINS.append(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41bef69a-a218-44cf-9633-64b37658c582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency distribution saved!\n"
     ]
    }
   ],
   "source": [
    "# Let's do the counting!\n",
    "\n",
    "subtotals = [docbin_counter(docbin, nlp) for docbin in DOCBINS] # we will apply a counting function to each docbin here\n",
    "total = Counter() # we set up a blank counter which will consolidate all the docbin-level totals\n",
    "for subtotal in subtotals: # this loop does the counting\n",
    "    total.update(subtotal)\n",
    "fdist = fdist2table(total, savename = \"../../output_data/words_frequency_experience.xlsx\") # save it to excel, you can make the filename whatever you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0d6ce9d-dc5a-4752-9fbe-b516f95fb614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stress</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>1072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>stress</td>\n",
       "      <td>VERB</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26236</th>\n",
       "      <td>stress</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word label  count\n",
       "11     stress  NOUN   1072\n",
       "1389   stress  VERB    247\n",
       "26236  stress   ADJ      1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.query(\"word == 'stress'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806b2014-27f5-49aa-89ec-51ebb1f68615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show top 20 NOUN, VERB, ADJ in a table\n",
    "fdist.query(\"label == 'VERB'\").sort_values(\"count\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd7cf0-c136-440b-94e3-1deb9c65244a",
   "metadata": {},
   "source": [
    "## Compare professionals vs non-professional Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edbbd46d-ea65-466c-b1c3-52213d3ce1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof_count_df = pd.read_excel(\"../../output_data/words_frequency_professional.xlsx\")[['word', 'label', 'count']]\\\n",
    ".rename(columns = {'count': 'count_prof'})\n",
    "exp_count_df = pd.read_excel(\"../../output_data/words_frequency_experience.xlsx\")[['word', 'label', 'count']]\\\n",
    ".rename(columns = {'count': 'count_amat'})\n",
    "\n",
    "tot_count_df = prof_count_df.merge(exp_count_df, left_on = ['word', 'label'], right_on = ['word', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9237c72a-2c15-48f7-a560-2afe3ac18f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "professional_words = ['illness',\n",
    "                 'treatement',\n",
    "                 'disease',\n",
    "                 'analysis',\n",
    "                 'cure',\n",
    "                 'hospitalization',\n",
    "                 'medication',\n",
    "                 'medicine',\n",
    "                 'operation',\n",
    "                 'prescription',\n",
    "                 'healing',\n",
    "                 'therapy',\n",
    "                 'knowledge',\n",
    "                 'learn',\n",
    "                 'psichologist'     \n",
    "                ]\n",
    "\n",
    "experience_words = ['life',\n",
    "                 'growth',\n",
    "                 'love',\n",
    "                 'comprehension',\n",
    "                 'judgement',\n",
    "                 'inspiration',\n",
    "                 'gratitude',\n",
    "                 'story',\n",
    "                 'habit'\n",
    "                 'day',\n",
    "                 'selfcare',\n",
    "                 'happiness'\n",
    "                ]\n",
    "                 \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62f5382e-c158-4fef-90a5-2ac73714a7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "      <th>count_prof</th>\n",
       "      <th>count_amat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2778</th>\n",
       "      <td>comprehension</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>7</td>\n",
       "      <td>10960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>gratitude</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>156</td>\n",
       "      <td>6450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>gratitude</td>\n",
       "      <td>VERB</td>\n",
       "      <td>48</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>growth</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>173</td>\n",
       "      <td>15883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>happiness</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>256</td>\n",
       "      <td>6796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>inspiration</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>434</td>\n",
       "      <td>6106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>judgement</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>128</td>\n",
       "      <td>16372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>judgement</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4575</th>\n",
       "      <td>judgement</td>\n",
       "      <td>VERB</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>life</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>3826</td>\n",
       "      <td>19649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8121</th>\n",
       "      <td>love</td>\n",
       "      <td>AUX</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>love</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>1547</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8285</th>\n",
       "      <td>love</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>love</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1648</td>\n",
       "      <td>7906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4823</th>\n",
       "      <td>selfcare</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7627</th>\n",
       "      <td>selfcare</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4024</th>\n",
       "      <td>selfcare</td>\n",
       "      <td>VERB</td>\n",
       "      <td>410</td>\n",
       "      <td>1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>story</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>588</td>\n",
       "      <td>1882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word  label  count_prof  count_amat\n",
       "2778  comprehension   NOUN           7       10960\n",
       "2624      gratitude   NOUN         156        6450\n",
       "4081      gratitude   VERB          48         511\n",
       "2665         growth   NOUN         173       15883\n",
       "2628      happiness   NOUN         256        6796\n",
       "294     inspiration   NOUN         434        6106\n",
       "2027      judgement   NOUN         128       16372\n",
       "4993      judgement  PROPN           3           2\n",
       "4575      judgement   VERB           4          10\n",
       "530            life   NOUN        3826       19649\n",
       "8121           love    AUX           1           2\n",
       "3076           love   NOUN        1547         562\n",
       "8285           love  PROPN           5           3\n",
       "551            love   VERB        1648        7906\n",
       "4823       selfcare    ADJ          11           6\n",
       "7627       selfcare   NOUN          21          26\n",
       "4024       selfcare   VERB         410        1135\n",
       "215           story   NOUN         588        1882"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_count_df[tot_count_df.word.isin(experience_words)].sort_values(by=['word', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "db71e89b-519d-42e2-be9e-51f606d7e5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "      <th>count_exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>life</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>19649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>17477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>judgement</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>16372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>growth</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>15883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12694</th>\n",
       "      <td>⁣</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>13100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17303</th>\n",
       "      <td>𝐬𝐨𝐦𝐞</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17304</th>\n",
       "      <td>⁣\\nyour gut produce up to</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17305</th>\n",
       "      <td>olympic</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17306</th>\n",
       "      <td>Googling</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28823</th>\n",
       "      <td>Pomeranian</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28824 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            word  label  count_exp\n",
       "63                          life   NOUN      19649\n",
       "5                        anxiety   NOUN      17477\n",
       "11538                  judgement   NOUN      16372\n",
       "2640                      growth   NOUN      15883\n",
       "12694                          ⁣   NOUN      13100\n",
       "...                          ...    ...        ...\n",
       "17303                       𝐬𝐨𝐦𝐞  PROPN          1\n",
       "17304  ⁣\\nyour gut produce up to   VERB          1\n",
       "17305                    olympic    ADJ          1\n",
       "17306                   Googling  PROPN          1\n",
       "28823                 Pomeranian  PROPN          1\n",
       "\n",
       "[28824 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_count_df.sort_values(by ='count_exp',ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d467c49-2246-43f9-8e70-62c000b69b88",
   "metadata": {},
   "source": [
    "## Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2a207-ad7a-41b0-beb4-a3926bb737f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_df = collocator_main(\n",
    "    \n",
    "    (\"neurotransmitter\", \"NOUN\"),   ## you can change this, but pay attention to the format!\n",
    "    DOCBINS, \n",
    "    nlp, \n",
    "    total, \n",
    "    window_size = 3,   ## you can change this, we will discuss this in class (window size = 2 -> just next to the one we have)\n",
    "    remove_stopwords = True ## option for advanced use\n",
    "    \n",
    ")\n",
    "\n",
    "# this will look a little clunky, and as you can see, some minor errors need fixing; this is the most complicated computation.\n",
    "\n",
    "cl_df.to_excel(\"../../output_data/words_collocation.xlsx\", index=True, engine = \"xlsxwriter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a88ba9-b8ec-424c-8310-0d4348444b12",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0065e5-f2a5-48f5-97ac-0ea60fcac5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hits = concordancer(\n",
    "    DOCBINS,\n",
    "    (\"anxious\",\"ADJ\"),\n",
    "    5, # this is how on either side to look for (window size)\n",
    "    nlp,\n",
    "    sample_size = 20,\n",
    "    label = \"anxious|ADJ\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc94f75-8977-46eb-88be-f461343918ab",
   "metadata": {},
   "source": [
    "#### Observe: apparently most keywords are only used as hashtgas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24ded7-edc0-4648-8872-f937d21777c8",
   "metadata": {},
   "source": [
    "## Keyness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2c9a5-d828-4fc8-b711-291a9eaf0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute keyness, first we need a frequency distribution of a subset of our posts to compare with the total.\n",
    "\n",
    "keyness_fdist = sliced_docbin_word_counter(\n",
    "    \n",
    "    DOCBINS,\n",
    "    df,\n",
    "    nlp,\n",
    "    slice_value = \"healthanxietycoach\", # we are slicing on a value, so here we can put in the username we want\n",
    "    slice_variable = \"author\", # this tells us which column of our data table to find the value above\n",
    "    remove_stopwords = True,\n",
    "    docbin_size = DOCBIN_SIZE\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b8312-2a26-416e-8aac-54141f4ddbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then it is just a matter of statistics! We will use a Chi-Squared statistic and the PDIFF statistic proposed by Gabrielatos (2018)\n",
    "\n",
    "kn_df = keyness_chi_sq(keyness_fdist, total, savename = \"../../output_data/keywords_chisq.xlsx\")\n",
    "kn_df = keyness_pdiff(keyness_fdist, total, savename = \"../../output_data/keywords_pdiff.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c871b-f453-4e1b-b4b3-995d96364006",
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_df.sort_values(\"pdiff\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596c28e-ac19-4a3b-b8ee-cd168066a93c",
   "metadata": {},
   "source": [
    "## Hashtag and Social Media Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188ebe4-4a75-4dc7-8b0b-4c445cd4a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_fdist = hashtag_counter(documents, savename = \"../../output_data/hashtags.xlsx\") # this takes care of everything and makes an excel spreadsheet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
