{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017cb975-9631-4546-8739-908ebe32365f",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a2eacb-8e50-4455-b8cc-9577019c7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from drs_corpora import * #import helper functions\n",
    "nlp = spacy.load(\"en_core_web_sm\") # load the spaCy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb55ef-26ff-40cd-9a25-8b557f9984cf",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69784020-fd2b-47f4-a63e-bc4d800b6d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to data folder: ../intermediate_data\n"
     ]
    }
   ],
   "source": [
    "INNTERMEDIATE_DATA = \"intermediate_data\" #we split up all the documents into a bunch of \".db\" files, or \"docbin\" files that hold our text data.\n",
    "DOCBIN_SIZE = None\n",
    "SAVEDIR = \"../\" + INNTERMEDIATE_DATA\n",
    "\n",
    "try:\n",
    "    os.mkdir(SAVEDIR)\n",
    "    print(\"Directory created. Path to data folder:\", SAVEDIR)\n",
    "except FileExistsError:\n",
    "    print(\"Path to data folder:\", SAVEDIR)\n",
    "DOCBIN_FILENAME_PATTERN = SAVEDIR + \"/\" + INNTERMEDIATE_DATA + \"_docbin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c74e19-22dc-4269-bd4e-44e02f16c5fa",
   "metadata": {},
   "source": [
    "## Read Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcb731c3-be28-4e12-a54d-f881d9dea7ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2385565993.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    {profiles_category_dict =\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "{profiles_category_dict = \n",
    "\n",
    "{\"anxiety.positive\": \"experience\",\n",
    " \"anxiety_fightermum\": \"experience\",\n",
    " \"your.anxious.therapist\": \"professional\"\n",
    ",\"drkirren\": \"professional\"\n",
    ",\"drjulie\":\t\"professional\"\n",
    ",\"the.anxious.truth\": \"professional\n",
    ",\"cherellethinks\": \"professional\"\n",
    ",\"theanxietyhealer\": \"professional\"\n",
    ",\"myeasytherapy\": \"professional\"\n",
    ",\"dearmyanxiety\": \"experience\"\n",
    ",\"anxietyjosh\": \"experience\"\n",
    ",\"healyournervoussystem\": \"professional\"\n",
    ",\"thehealingtherapist\": \"professional\"\n",
    ",\"anxiety_fitness\": \"experience\"\n",
    ",\"healthanxietycoach\": \"professional\"\n",
    ",\"ahealthypush\": \"professional\"\n",
    ",\"_peacefromwithin\": \"professional\"\n",
    ",\"honestlyholistic\": \"experience\"\n",
    ",\"health_anxiety\": \"experience\"\n",
    ",\"zowhy_coaching\": \"experience\"\n",
    ",\"becdiekman\": \"experience\"\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a13e986e-73aa-48ea-a3b2-d87fda4485d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../input_data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m json_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofiles_category.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjson_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.T.reset_index()\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/uva-drs-mental-health-assignment-BL2gK8Gv-py3.9/lib/python3.9/site-packages/pandas/io/json/_json.py:804\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/uva-drs-mental-health-assignment-BL2gK8Gv-py3.9/lib/python3.9/site-packages/pandas/io/json/_json.py:1012\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m   1011\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1012\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/uva-drs-mental-health-assignment-BL2gK8Gv-py3.9/lib/python3.9/site-packages/pandas/io/json/_json.py:1040\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m   1038\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1040\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/uva-drs-mental-health-assignment-BL2gK8Gv-py3.9/lib/python3.9/site-packages/pandas/io/json/_json.py:1173\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1173\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1176\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/uva-drs-mental-health-assignment-BL2gK8Gv-py3.9/lib/python3.9/site-packages/pandas/io/json/_json.py:1366\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1362\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1366\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m     )\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1369\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1370\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1371\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1372\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "# Specify the path to the folder containing CSV files\n",
    "folder_path = '../input_data/'\n",
    "json_name = 'profiles_category.json'\n",
    "\n",
    "df = pd.read_json(folder_path + json_name, lines=True)#.T.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fea889c-99fa-45f5-bcb4-aedb8458b10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26791\n",
      "26692\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the folder containing CSV files\n",
    "folder_path = '../input_data/'\n",
    "\n",
    "# Initialize an empty list to store the DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Construct the full path to the CSV file\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Append the DataFrame to the list\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(len(df))\n",
    "df = df[df[\"body\"].isna() == False]\n",
    "documents = list(df[\"body\"])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8efb92-3398-46cc-bf0a-0660cb70eaa3",
   "metadata": {},
   "source": [
    "## Tokenization, Lemmatization, Named Entity Recognition, and POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a55cacb-93a5-4491-a177-0740f6315b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docbin size for this project:  1000 \n",
      "Please take note of the docbin size if you want to come back to your project and not process the data again.\n",
      "Saving chunk as:  ../intermediate_data/intermediate_data_docbin_docbin_0.db\n",
      "Saving chunk as:  ../intermediate_data/intermediate_data_docbin_docbin_1.db\n",
      "Saving chunk as:  ../intermediate_data/intermediate_data_docbin_docbin_2.db\n",
      "Saving chunk as:  ../intermediate_data/intermediate_data_docbin_docbin_3.db\n",
      "Saving chunk as:  ../intermediate_data/intermediate_data_docbin_docbin_4.db\n",
      "CPU times: user 13.6 s, sys: 2.58 s, total: 16.2 s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlp.add_pipe(\"merge_entities\") # this makes the tokens cleaner later\n",
    "\n",
    "N_PROCESS = 10 # you can change this depending on how many cores your CPU has (eg. a Mac M1 has 8 cores, so you can use up to 7 here)\n",
    "BATCH_SIZE = 100 # this depends on how much RAM you have. If this process hangs or crashes, you may want to reduce batch size (how many docs each core will process in one chunk)\n",
    "DOCBIN_SIZE = N_PROCESS * BATCH_SIZE\n",
    "\n",
    "print(\"docbin size for this project: \", str(DOCBIN_SIZE), \"\\nPlease take note of the docbin size if you want to come back to your project and not process the data again.\")\n",
    "\n",
    "for i, chunk in enumerate(chunker(documents, DOCBIN_SIZE)): # chopping our dataset into chunks. We don't need this in our toy example, but we do for large datsets; change from 100 if you want\n",
    "    doc_bin = DocBin(store_user_data = True) # create a docbin for our chunk\n",
    "    for doc in nlp.pipe(chunk, n_process = N_PROCESS, batch_size = BATCH_SIZE): # process our documents, you can play with n_process and batch_size depending on your CPU and RAM\n",
    "        doc_bin.add(doc) # save the document to our docbin\n",
    "    chunk_name = DOCBIN_FILENAME_PATTERN + \"_docbin_\" + str(i) + \".db\" # make a nice filename for each chunk\n",
    "    print(\"Saving chunk as: \", chunk_name) # display progress\n",
    "    doc_bin.to_disk(chunk_name) # save docbin for chunk to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9ba50-2e29-4892-88db-4d466227a971",
   "metadata": {},
   "source": [
    "## Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b505c0-dd40-440a-ba2c-4e231d89fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we can do anything, we need to get a list of all the names of the docbins, in order, thats what happens here\n",
    "\n",
    "docbin_folder = SAVEDIR + \"/*.db\" # you might have to change this if you use this notebook for a different folder\n",
    "docbins = [docbin for docbin in glob.iglob(docbin_folder)]\n",
    "DOCBINS = list()\n",
    "for i in range(0, len(docbins), 1):\n",
    "    db_path = DOCBIN_FILENAME_PATTERN + \"_docbin_\" + str(i) + \".db\" # here the naming pattern of the docbins is hard coded, so you may have to change this if you apply it to another project\n",
    "    DOCBINS.append(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41bef69a-a218-44cf-9633-64b37658c582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency distribution saved!\n"
     ]
    }
   ],
   "source": [
    "# Let's do the counting!\n",
    "\n",
    "subtotals = [docbin_counter(docbin, nlp) for docbin in DOCBINS] # we will apply a counting function to each docbin here\n",
    "total = Counter() # we set up a blank counter which will consolidate all the docbin-level totals\n",
    "for subtotal in subtotals: # this loop does the counting\n",
    "    total.update(subtotal)\n",
    "fdist = fdist2table(total, savename = \"../output_data/words_frequency.xlsx\") # save it to excel, you can make the filename whatever you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "806b2014-27f5-49aa-89ec-51ebb1f68615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>label</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>#grateful #</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>6196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>✨</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>thinkpositive</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>authentic</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>tiredmum</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738</th>\n",
       "      <td>controlwhatyoucan</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1826</th>\n",
       "      <td>showup</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>great</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>ichoose</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>good</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>thankful</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>mentalhealthblogger</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>grateful</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>#internaldiaglogue #</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>well</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>💪</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>happy</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>mental</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>#yourfeelingsmatt #</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>small</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      word label  count\n",
       "19             #grateful #   ADJ   6196\n",
       "93                       ✨   ADJ    283\n",
       "1505         thinkpositive   ADJ    217\n",
       "2227             authentic   ADJ    201\n",
       "1600              tiredmum   ADJ    134\n",
       "1738     controlwhatyoucan   ADJ    112\n",
       "1826                showup   ADJ    107\n",
       "86                   great   ADJ    103\n",
       "2169               ichoose   ADJ    101\n",
       "351                   good   ADJ    100\n",
       "393               thankful   ADJ     87\n",
       "1502   mentalhealthblogger   ADJ     86\n",
       "0                 grateful   ADJ     75\n",
       "1436  #internaldiaglogue #   ADJ     75\n",
       "42                    well   ADJ     69\n",
       "214                      💪   ADJ     67\n",
       "686                  happy   ADJ     61\n",
       "234                 mental   ADJ     58\n",
       "2426   #yourfeelingsmatt #   ADJ     55\n",
       "575                  small   ADJ     45"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show top 20 NOUN, VERB, ADJ in a table\n",
    "fdist.query(\"label == 'ADJ'\").sort_values(\"count\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d467c49-2246-43f9-8e70-62c000b69b88",
   "metadata": {},
   "source": [
    "## Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e2a207-ad7a-41b0-beb4-a3926bb737f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered, printing contingency table values...\n",
      " A: 5128  B: 12013  C: 0  D: 257300\n",
      "Error encountered, printing contingency table values...\n",
      " A: 2274  B: 14867  C: 0  D: 257300\n",
      "Error encountered, printing contingency table values...\n",
      " A: 13  B: 0  C: 17128  D: 257300\n",
      "Error encountered, printing contingency table values...\n",
      " A: 15  B: 0  C: 17126  D: 257300\n",
      "Error encountered, printing contingency table values...\n",
      " A: 251  B: 16890  C: 0  D: 257300\n",
      "Error encountered, printing contingency table values...\n",
      " A: 13  B: 0  C: 17128  D: 257300\n",
      "Error encountered, printing contingency table values...\n",
      " A: 33  B: 17108  C: 0  D: 257300\n",
      "Error encountered, printing contingency table values...\n",
      " A: 8  B: 0  C: 17133  D: 257300\n"
     ]
    }
   ],
   "source": [
    "cl_df = collocator_main(\n",
    "    \n",
    "    (\"life\", \"NOUN\"),   ## you can change this, but pay attention to the format!\n",
    "    DOCBINS, \n",
    "    nlp, \n",
    "    total, \n",
    "    window_size = 3,   ## you can change this, we will discuss this in class (window size = 2 -> just next to the one we have)\n",
    "    remove_stopwords = True ## option for advanced use\n",
    "    \n",
    ")\n",
    "\n",
    "# this will look a little clunky, and as you can see, some minor errors need fixing; this is the most complicated computation.\n",
    "\n",
    "cl_df.to_excel(\"../output_data/words_collcation.xlsx\", index=True, engine = \"xlsxwriter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a88ba9-b8ec-424c-8310-0d4348444b12",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc0065e5-f2a5-48f5-97ac-0ea60fcac5b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary +: 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_hits \u001b[38;5;241m=\u001b[39m \u001b[43mconcordancer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDOCBINS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomprehension\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNOUN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# this is how on either side to look for (window size)\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomprehension|NOUN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/uva-drs-mental-health-assignment/src/drs_corpora.py:313\u001b[0m, in \u001b[0;36mconcordancer\u001b[0;34m(docbins, query, window, nlp, sample_size, label)\u001b[0m\n\u001b[1;32m    311\u001b[0m hits \u001b[38;5;241m=\u001b[39m [compute_concordance(docbin, query, window, nlp) \u001b[38;5;28;01mfor\u001b[39;00m docbin \u001b[38;5;129;01min\u001b[39;00m docbins]\n\u001b[1;32m    312\u001b[0m all_hits \u001b[38;5;241m=\u001b[39m flatten(hits)\n\u001b[0;32m--> 313\u001b[0m \u001b[43mgenerate_html_hits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_hits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_hits\n",
      "File \u001b[0;32m~/Projects/uva-drs-mental-health-assignment/src/drs_corpora.py:283\u001b[0m, in \u001b[0;36mgenerate_html_hits\u001b[0;34m(hits, sample_size, label)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_html_hits\u001b[39m(hits, sample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    282\u001b[0m     html_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m--> 283\u001b[0m     outfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../output_data/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m \u001b[38;5;241m+\u001b[39m label \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_concordance.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(hits):\n\u001b[1;32m    285\u001b[0m         sample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(hits)\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary +: 'str'"
     ]
    }
   ],
   "source": [
    "all_hits = concordancer(\n",
    "    DOCBINS,\n",
    "    (\"comprehension\",\"NOUN\"),\n",
    "    5, # this is how on either side to look for (window size)\n",
    "    nlp,\n",
    "    sample_size = 20,\n",
    "    label = \"comprehension|NOUN\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc94f75-8977-46eb-88be-f461343918ab",
   "metadata": {},
   "source": [
    "#### Observe: apparently most keywords are only used as hashtgas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24ded7-edc0-4648-8872-f937d21777c8",
   "metadata": {},
   "source": [
    "## Keyness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2c9a5-d828-4fc8-b711-291a9eaf0f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute keyness, first we need a frequency distribution of a subset of our posts to compare with the total.\n",
    "\n",
    "keyness_fdist = sliced_docbin_word_counter(\n",
    "    \n",
    "    DOCBINS,\n",
    "    df,\n",
    "    nlp,\n",
    "    slice_value = \"anxiety_fightermum\", # we are slicing on a value, so here we can put in the username we want\n",
    "    slice_variable = \"author\", # this tells us which column of our data table to find the value above\n",
    "    remove_stopwords = True,\n",
    "    docbin_size = DOCBIN_SIZE\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3b8312-2a26-416e-8aac-54141f4ddbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then it is just a matter of statistics! We will use a Chi-Squared statistic and the PDIFF statistic proposed by Gabrielatos (2018)\n",
    "\n",
    "kn_df = keyness_chi_sq(keyness_fdist, total, savename = \"../output_data/keywords_chisq.xlsx\")\n",
    "kn_df = keyness_pdiff(keyness_fdist, total, savename = \"../output_data/keywords_pdiff.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c871b-f453-4e1b-b4b3-995d96364006",
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_df.sort_values(\"pdiff\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596c28e-ac19-4a3b-b8ee-cd168066a93c",
   "metadata": {},
   "source": [
    "## Hashtag and Social Media Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188ebe4-4a75-4dc7-8b0b-4c445cd4a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_fdist = hashtag_counter(documents, savename = \"../output_data/hashtags.xlsx\") # this takes care of everything and makes an excel spreadsheet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
